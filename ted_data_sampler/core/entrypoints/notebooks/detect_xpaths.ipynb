{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML XPath Extractor\n",
    "\n",
    "This notebook processes XML files from an input directory, extracts XPaths (unique or all), and saves the results to an output directory with detailed statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "INPUT_FOLDER_PATH: Path = Path(\"../../../../tests/test_data/test_eform_notices/\")\n",
    "OUTPUT_FOLDER_PATH: Path = Path(\"../../../../output\")\n",
    "EXTRACT_UNIQUE_XPATHS: bool = True\n",
    "GENERATE_STATISTICS: bool = True\n",
    "\n",
    "if not INPUT_FOLDER_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Input folder does not exist: {INPUT_FOLDER_PATH}\")\n",
    "\n",
    "OUTPUT_FOLDER_PATH.mkdir(parents=True, exist_ok=True)\n",
    "STATS_FOLDER_PATH = OUTPUT_FOLDER_PATH / 'statistics'\n",
    "STATS_FOLDER_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "input_folders = [item.name for item in INPUT_FOLDER_PATH.iterdir()]\n",
    "print(input_folders)\n",
    "print(STATS_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_paths(element: ET.Element,\n",
    "                  current_path: str = \"\",\n",
    "                  paths: Optional[List[str]] = None) -> List[str]:\n",
    "    if paths is None:\n",
    "        paths = []\n",
    "\n",
    "    tag = element.tag\n",
    "    if '}' in tag:\n",
    "        tag = tag.split('}')[1]\n",
    "\n",
    "    new_path = f\"{current_path}/{tag}\" if current_path else tag\n",
    "\n",
    "    if EXTRACT_UNIQUE_XPATHS:\n",
    "        if new_path not in paths:\n",
    "            paths.append(new_path)\n",
    "    else:\n",
    "        paths.append(new_path)\n",
    "\n",
    "    for attr in element.attrib.keys():\n",
    "        attr_path = f\"{new_path}/@{attr}\"\n",
    "        if EXTRACT_UNIQUE_XPATHS:\n",
    "            if attr_path not in paths:\n",
    "                paths.append(attr_path)\n",
    "        else:\n",
    "            paths.append(attr_path)\n",
    "\n",
    "    for child in element:\n",
    "        get_all_paths(child, new_path, paths)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml_files(input_dir: Path) -> Tuple[List[str], Dict[str, List[str]]]:\n",
    "    all_paths: List[str] = []\n",
    "    file_path_map: Dict[str, List[str]] = {}\n",
    "\n",
    "    xml_files = list(input_dir.glob('*.xml'))\n",
    "\n",
    "    print(f\"Found {len(xml_files)} XML files to process\")\n",
    "    print(f\"{'Unique' if EXTRACT_UNIQUE_XPATHS else 'All'} XPaths will be extracted\")\n",
    "\n",
    "    for xml_file in tqdm(xml_files, desc=\"Processing XML files\"):\n",
    "        try:\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            file_paths = get_all_paths(root)\n",
    "            file_path_map[xml_file.name] = file_paths\n",
    "\n",
    "            if EXTRACT_UNIQUE_XPATHS:\n",
    "                all_paths.extend([p for p in file_paths if p not in all_paths])\n",
    "            else:\n",
    "                all_paths.extend(file_paths)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {xml_file.name}: {str(e)}\")\n",
    "\n",
    "    return all_paths, file_path_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XPathStatisticsAnalyzer:\n",
    "    def __init__(self, all_paths: List[str], file_path_mapping: Dict[str, List[str]]):\n",
    "        self.all_paths = all_paths\n",
    "        self.file_path_mapping = file_path_mapping\n",
    "        self.unique_paths = list(dict.fromkeys(all_paths))\n",
    "\n",
    "    def get_path_depth_stats(self) -> pd.DataFrame:\n",
    "        def get_depth(xpath: str) -> int:\n",
    "            return len([p for p in xpath.split('/') if p and not p.startswith('@')])\n",
    "\n",
    "        depths = [get_depth(path) for path in self.unique_paths]\n",
    "        depth_stats = pd.DataFrame({\n",
    "            'xpath': self.unique_paths,\n",
    "            'depth': depths,\n",
    "            'occurrence_count': [self.all_paths.count(p) for p in self.unique_paths]\n",
    "        })\n",
    "        return depth_stats\n",
    "\n",
    "    def get_attribute_analysis(self) -> pd.DataFrame:\n",
    "        attr_data = []\n",
    "        for path in self.unique_paths:\n",
    "            if '@' in path:\n",
    "                element_path = path.split('/@')[0]\n",
    "                attr_name = path.split('/@')[1]\n",
    "                count = self.all_paths.count(path)\n",
    "                attr_data.append({\n",
    "                    'element_path': element_path,\n",
    "                    'attribute': attr_name,\n",
    "                    'occurrence_count': count,\n",
    "                    'files_count': sum(1 for paths in self.file_path_mapping.values() if path in paths)\n",
    "                })\n",
    "        return pd.DataFrame(attr_data)\n",
    "\n",
    "    def get_path_patterns(self) -> pd.DataFrame:\n",
    "        pattern_data = []\n",
    "        for path in self.unique_paths:\n",
    "            elements = [p for p in path.split('/') if p and not p.startswith('@')]\n",
    "            pattern_data.append({\n",
    "                'xpath': path,\n",
    "                'element_count': len(elements),\n",
    "                'has_attributes': '@' in path,\n",
    "                'terminal_element': elements[-1] if elements else '',\n",
    "                'occurrence_count': self.all_paths.count(path),\n",
    "                'files_count': sum(1 for paths in self.file_path_mapping.values() if path in paths)\n",
    "            })\n",
    "        return pd.DataFrame(pattern_data)\n",
    "\n",
    "    def get_file_complexity_metrics(self) -> pd.DataFrame:\n",
    "        metrics_data = []\n",
    "        for file, paths in self.file_path_mapping.items():\n",
    "            unique_paths = list(dict.fromkeys(paths))\n",
    "            metrics_data.append({\n",
    "                'file_name': file,\n",
    "                'total_paths': len(paths),\n",
    "                'unique_paths': len(unique_paths),\n",
    "                'max_depth': max(len(p.split('/')) for p in unique_paths),\n",
    "                'avg_depth': np.mean([len(p.split('/')) for p in unique_paths]),\n",
    "                'attribute_count': sum(1 for p in unique_paths if '@' in p),\n",
    "                'complexity_score': len(unique_paths) * np.mean([len(p.split('/')) for p in unique_paths])\n",
    "            })\n",
    "        return pd.DataFrame(metrics_data)\n",
    "\n",
    "    def get_path_relationships(self) -> pd.DataFrame:\n",
    "        relationships = []\n",
    "        for path in self.unique_paths:\n",
    "            parts = path.split('/')\n",
    "            for i in range(1, len(parts)):\n",
    "                parent = '/'.join(parts[:i])\n",
    "                child = '/'.join(parts[:i + 1])\n",
    "                if parent and child != path:\n",
    "                    relationships.append({\n",
    "                        'parent_path': parent,\n",
    "                        'child_path': child,\n",
    "                        'level_difference': i,\n",
    "                        'parent_occurrence': self.all_paths.count(parent),\n",
    "                        'child_occurrence': self.all_paths.count(child)\n",
    "                    })\n",
    "        return pd.DataFrame(relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XPathExtractor:\n",
    "    def __init__(self, input_path: Path, output_path: Path):\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.stats_path = output_path / 'statistics'\n",
    "        self.all_paths: List[str] = []\n",
    "        self.file_path_mapping: Dict[str, List[str]] = {}\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def save_results(self) -> None:\n",
    "        results_dir = self.output_path / f'results_{self.timestamp}'\n",
    "        results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        paths_df = pd.DataFrame(self.all_paths, columns=['xpath'])\n",
    "        if EXTRACT_UNIQUE_XPATHS:\n",
    "            output_name = 'unique_xpaths.csv'\n",
    "        else:\n",
    "            output_name = 'all_xpaths.csv'\n",
    "            paths_df['occurrence_count'] = paths_df['xpath'].map(\n",
    "                paths_df['xpath'].value_counts()\n",
    "            )\n",
    "\n",
    "        paths_csv_path = results_dir / output_name\n",
    "        paths_df.to_csv(paths_csv_path, index=False)\n",
    "\n",
    "        unique_paths = sorted(set(self.all_paths))\n",
    "        matrix_data = [\n",
    "            {'xpath': xpath, **{file: file_paths.count(xpath)\n",
    "                                for file, file_paths in self.file_path_mapping.items()}}\n",
    "            for xpath in unique_paths\n",
    "        ]\n",
    "        matrix_df = pd.DataFrame(matrix_data)\n",
    "        matrix_df.to_csv(results_dir / 'xpath_file_matrix.csv', index=False)\n",
    "\n",
    "        if GENERATE_STATISTICS:\n",
    "            stats_dir = results_dir / 'statistics'\n",
    "            stats_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            analyzer = XPathStatisticsAnalyzer(self.all_paths, self.file_path_mapping)\n",
    "\n",
    "            stats_files = {\n",
    "                'depth_analysis.csv': analyzer.get_path_depth_stats(),\n",
    "                'attribute_analysis.csv': analyzer.get_attribute_analysis(),\n",
    "                'path_patterns.csv': analyzer.get_path_patterns(),\n",
    "                'file_complexity.csv': analyzer.get_file_complexity_metrics(),\n",
    "                'path_relationships.csv': analyzer.get_path_relationships()\n",
    "            }\n",
    "\n",
    "            for filename, df in stats_files.items():\n",
    "                df.to_csv(stats_dir / filename, index=False)\n",
    "\n",
    "            self._generate_summary_report(stats_dir, stats_files)\n",
    "\n",
    "        print(f\"Results saved to: {results_dir}\")\n",
    "\n",
    "    def _generate_summary_report(self, stats_dir: Path, stats_files: Dict[str, pd.DataFrame]) -> None:\n",
    "        with open(stats_dir / 'summary_report.txt', 'w') as f:\n",
    "            f.write(f\"XPath Analysis Summary Report\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "            f.write(f\"Total number of XML files: {len(self.file_path_mapping)}\\n\")\n",
    "            f.write(f\"Total XPaths found: {len(self.all_paths)}\\n\")\n",
    "            f.write(f\"Unique XPaths: {len(set(self.all_paths))}\\n\\n\")\n",
    "\n",
    "            depth_stats = stats_files['depth_analysis.csv']\n",
    "            f.write(\"Depth Statistics:\\n\")\n",
    "            f.write(f\"Maximum depth: {depth_stats['depth'].max()}\\n\")\n",
    "            f.write(f\"Average depth: {depth_stats['depth'].mean():.2f}\\n\\n\")\n",
    "\n",
    "            attr_stats = stats_files['attribute_analysis.csv']\n",
    "            f.write(\"Attribute Statistics:\\n\")\n",
    "            f.write(f\"Total attributes found: {len(attr_stats)}\\n\")\n",
    "            f.write(\n",
    "                f\"Most common attributes: {', '.join(attr_stats['attribute'].value_counts().head().index.tolist())}\\n\\n\")\n",
    "\n",
    "            complexity = stats_files['file_complexity.csv']\n",
    "            f.write(\"File Complexity:\\n\")\n",
    "            f.write(f\"Most complex file: {complexity.loc[complexity['complexity_score'].idxmax(), 'file_name']}\\n\")\n",
    "            f.write(f\"Average complexity score: {complexity['complexity_score'].mean():.2f}\\n\")\n",
    "\n",
    "    def display_statistics(self) -> None:\n",
    "        print(\"=== Analysis Results ===\")\n",
    "        print(f\"Results directory: {self.output_path}/results_{self.timestamp}\")\n",
    "        print(f\"\\nBasic Statistics:\")\n",
    "        print(f\"- Total XML files processed: {len(self.file_path_mapping)}\")\n",
    "        print(f\"- Total {'unique ' if EXTRACT_UNIQUE_XPATHS else ''}XPaths found: {len(self.all_paths)}\")\n",
    "\n",
    "        if GENERATE_STATISTICS:\n",
    "            analyzer = XPathStatisticsAnalyzer(self.all_paths, self.file_path_mapping)\n",
    "\n",
    "            depth_stats = analyzer.get_path_depth_stats()\n",
    "            print(\"\\nDepth Statistics:\")\n",
    "            print(f\"- Maximum depth: {depth_stats['depth'].max()}\")\n",
    "            print(f\"- Average depth: {depth_stats['depth'].mean():.2f}\")\n",
    "\n",
    "            attr_stats = analyzer.get_attribute_analysis()\n",
    "            if not attr_stats.empty:\n",
    "                print(\"\\nAttribute Statistics:\")\n",
    "                print(f\"- Total attributes found: {len(attr_stats)}\")\n",
    "                print(\"- Top 5 most common attributes:\")\n",
    "                for attr, count in attr_stats['attribute'].value_counts().head().items():\n",
    "                    print(f\"  - {attr}: {count} occurrences\")\n",
    "\n",
    "            complexity = analyzer.get_file_complexity_metrics()\n",
    "            print(\"\\nFile Complexity Metrics:\")\n",
    "            most_complex = complexity.loc[complexity['complexity_score'].idxmax()]\n",
    "            print(f\"- Most complex file: {most_complex['file_name']}\")\n",
    "            print(f\"  - Complexity score: {most_complex['complexity_score']:.2f}\")\n",
    "            print(f\"  - Unique paths: {most_complex['unique_paths']}\")\n",
    "            print(f\"  - Max depth: {most_complex['max_depth']}\")\n",
    "\n",
    "    def process(self) -> None:\n",
    "        self.all_paths, self.file_path_mapping = process_xml_files(\n",
    "            self.input_path\n",
    "        )\n",
    "        self.save_results()\n",
    "        self.display_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = XPathExtractor(INPUT_FOLDER_PATH, OUTPUT_FOLDER_PATH)\n",
    "extractor.process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
